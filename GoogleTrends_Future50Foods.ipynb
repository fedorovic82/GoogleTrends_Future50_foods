{"cells":[{"cell_type":"code","source":["from pyspark.sql.functions import *\nfrom pyspark.sql.types import *\nimport random\nimport math\nfrom pytrends.request import TrendReq\n\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *\nimport pandas as pd\nfrom pytrends.request import TrendReq\nimport numpy as np\nimport warnings\nimport random # Choose random list element\nimport time\nimport math\nfrom random import randint\nfrom time import sleep"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["input_country_code      = ''\ninput_language          =  'en'\ninput_keyword_column    = 'keyword'\ninput_gt_period         = '2018-01-01 2018-02-01'\ninput_gt_category       = '0'\nnr_batches              = 1\nnr_top_keywords         = None\nlimit_final_list        = None\n\ninput_period            = 12 # period is the interval period in months\ninput_data_col          = 'week'\ninput_group_feature     = 'keyword' \ninput_rank_feature      = 'rank' \ninput_value_feature     = 'scores'\ninput_period_feature    = 'period'\ninput_start_period      = -2\ninput_end_period        = -1\n\ninput_growth_measure    = 'trend_scores'       # either trend_scores OR trend_rank\ninput_size_measure      = 'end_value_scores'     # either avg_scores / end_value_scores / avg_rank / end_value_rank"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["future_50_foods = [\"laver seaweed\",\"wakame seaweed\",\"adzuki beans\",\"black turtle beans\",\"broad beans\",\"fava beans\",\"bambara groundnuts\",\"bambara beans\",\"cowpeas\",\"lentils\",\"marama beans\",\n                   \"mung beans\",\"soy beans\",\"nopales\",\"amaranth\",\"buckwheat\",\"finger millet\",\"fonio\",\"khorasan wheat\",\"quinoa\",\"spelt\",\"teff\",\"wild rice\",\"pumpkin flowers\",\"okra\",\"orange tomatoes\",\n                   \"beet greens\",\"broccoli rabe\",\"kale\",\"moringa\",\"pak-choi\",\"bok-choy\",\"pumpking leaves\",\"red cabbage\",\"spinach\",\"watercress\",\"enoki mushrooms\",\"maitake mushrooms\",\n                   \"saffron milk cap mushrooms\",\"flax seeds\",\"hemp seeds\",\"sesame seeds\",\"walnuts\",\"black salsify\",\"parsley root\",\"white icicle radish \",\"winter radish\",\n                   \"alfalfa sprouts\",\"sprouted kidney beans\",\"sprouted chickpeas\",\"lotus root\",\"ube\",\"purple yam\",\"yam bean root\",\"jicama\",\"red indonesian sweet potatoes\",\"cilembu sweet potatoes\"]\n\ntemp_future_50_foods = [\"quinoa\",\"vegetarian butcher\", \"lentils\", \"black salsify\",\"walnuts\"]\n\ndf_future_50_foods = spark.createDataFrame(temp_future_50_foods, StringType()).dropDuplicates().withColumnRenamed('value', 'keyword')"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["def partition(lst, n): \n    division = len(lst) / float(n)\n    return [lst[int(math.ceil(division * i)): int(math.ceil(division * (i + 1)))] for i in range(n)]\n\ndef check_volume(keyword_list, country_code, period, google_cat):\n    pytrends = TrendReq(hl='en-US', tz = 60, timeout=(10,25))  # hl = host language, tz = timezone offset (for example US CST is '360', Western European Summer time is '60')\n \n    output = []\n    for term in keyword_list:\n        # Create payload and capture API tokens. Only needed for interest_over_time(), interest_by_region() & related_queries()\n        sleep(randint(1, 3))\n        pytrends.build_payload([term], cat=google_cat, geo=country_code, gprop='', timeframe=period)  # up to 5 items at a time\n        tmp_df_gtrends = pytrends.interest_over_time()\n        if tmp_df_gtrends.empty:\n            warnings.warn('Not sufficient data available for keyword %s in %s' % (term, country_code))\n        else:\n            output.append(term)\n    return output\n  \ndef search_pairs(list_pairs, country_code, period, baseline, google_cat): # Search a list of keyword pairs and calculate the overall relative imporatance based on aggregated search volumes.\n    pytrends = TrendReq(hl='en-US', tz = 60)  # hl = host language, tz = timezone offset (for example US CST is '360', Western European Summer time is '60')\n \n    unsorted_df = []\n    for term in list_pairs:\n        # Create payload and capture API tokens. Only needed for interest_over_time(), interest_by_region() & related_queries()\n        sleep(randint(1, 3))\n        pytrends.build_payload(term, cat=google_cat, timeframe=period, geo=country_code, gprop='')  # up to 5 items at a time\n        if term == [baseline, baseline]:\n            importance = 1\n        else:\n            tmp_df_gtrends = pytrends.interest_over_time()\n            if tmp_df_gtrends.empty:\n                warnings.warn('Not sufficient data available for the combination %s in %s' % (term, country_code))\n                importance = np.NaN\n            else:\n                importance = tmp_df_gtrends[term[0]].sum() / tmp_df_gtrends[term[1]].sum()\n        unsorted_df.append(importance)\n    return unsorted_df\n  \ndef make_pairs_with_baseline(keyword_list, baseline):  # Create a list with in each element a list of 2 (nested list)\n    double_terms = []\n    for item in keyword_list: \n       double_terms.append([item, baseline])\n    print(double_terms)\n    return double_terms\n\ndef make_pairs_shifted(keyword_list):\n    search_terms_shifted = keyword_list[1:]\n    search_terms_shifted.extend(['this term will be removed'])\n    \n    double_terms = []\n    for item in range(0,len(keyword_list)):\n       double_terms.append([keyword_list[item], search_terms_shifted[item]])\n    double_terms = double_terms[:-1] # Remove last pair\n    return double_terms\n  \ndef get_raw_scores(keyword_list_sorted, country_code, period, google_cat):\n    double_terms = make_pairs_shifted(keyword_list_sorted)\n    print('double terms:', double_terms)\n    # Login to Google. Only need to run this once, the rest of requests will use the same session.\n    pytrends = TrendReq(hl='en-US', tz=60) \n    \n    raw_data = pd.DataFrame(columns= ['week', 'ref_word', 'keyword', 'value_ref_word', 'value_keyword', 'id_ref_word', 'id_keyword'])\n    iter=1\n    for term in double_terms:\n        # Create payload and capture API tokens. Only needed for interest_over_time(), interest_by_region() & related_queries()\n        sleep(randint(1, 3))\n        pytrends.build_payload(term, cat=google_cat, timeframe=period, geo=country_code, gprop='')\n\n        # Interest Over Time\n        tmp_df_gtrends = pytrends.interest_over_time()\n        if tmp_df_gtrends.empty:\n            warnings.warn('Not sufficient data available for the combination %s in %s' % (term, country_code))\n        else:\n            lst_week_epoch = tmp_df_gtrends.index.values.tolist()\n            lst_week = []\n            for item in lst_week_epoch:\n                item = int(item / 1000000000)\n                item = time.strftime('%Y%m%d', time.localtime(item))\n                lst_week.append(item)\n            tmp_df_gtrends = tmp_df_gtrends.assign(week=lst_week)\n            df_gtrends_headers = list(tmp_df_gtrends.columns.values)\n            df_gtrends_headers = [x for x in df_gtrends_headers if x != 'isPartial']\n            tmp_df_gtrends = tmp_df_gtrends[df_gtrends_headers]\n            tmp_df_gtrends = pd.melt(tmp_df_gtrends, id_vars=['week', term[1]], var_name='ref_word')\n            tmp_df_gtrends.rename(columns={'value':'value_ref_word'}, inplace=True)\n            tmp_df_gtrends = pd.melt(tmp_df_gtrends, id_vars=['week', 'value_ref_word', 'ref_word'], var_name='keyword')\n            tmp_df_gtrends.rename(columns={'value':'value_keyword'}, inplace=True)\n            tmp_df_gtrends = tmp_df_gtrends.assign(country_code=country_code)\n            tmp_df_gtrends = tmp_df_gtrends.assign(id_ref_word=iter, id_keyword=iter+1)\n            raw_data = pd.concat([raw_data, tmp_df_gtrends])\n        iter = iter + 1\n        \n    # Add a part for the most popular keyword\n    tmp = raw_data[raw_data['id_ref_word'] == 1]\n    tmp['id_keyword'] = tmp['id_ref_word'].values\n    tmp['keyword'] = tmp['ref_word'].values\n    tmp['value_keyword'] = tmp['value_ref_word'].values\n    raw_data = pd.concat([tmp, raw_data])\n    return raw_data\n  \ndef get_relative_rank(keyword_list, country_code, period, google_cat):\n    baseline         = random.choice(keyword_list) \n    print(\"The baseline keyword is \" + str(baseline))\n    double_terms     = make_pairs_with_baseline(keyword_list, baseline)\n    \n    df                = pd.DataFrame(columns = ['score', 'search_term'])\n    df['search_term'] = keyword_list\n    df['score']       = search_pairs(double_terms, country_code, period, baseline, google_cat)\n    \n    # Create a sorted list of terms\n    df                 = df.sort_values('score', ascending = False).reset_index(drop=True)\n    sorted_list        = df['search_term'].tolist()\n    print('Search Terms Sorted by Volume')\n    print(sorted_list)\n    return sorted_list\n  \n# Pandas DF to Spark DF\ndef pandas_to_spark(pandas_df):\n    #Create PySpark DataFrame Schema\n    spark_df = sqlContext.createDataFrame(pandas_df)\n    return spark_df\n\n  \ndef get_raw_google_trends_batch_search(df, keyword_feature, country_code, period, google_cat, nr_batches, nr_top_keywords = None, limit_final_list = None): \n    keywords_with_volume       = []\n    df_pd                      = df.toPandas()\n    keyword_list               = df_pd[keyword_feature].tolist()\n    keyword_list               = list(set(keyword_list)) # Get unqiue categories\n    random.shuffle(keyword_list)\n    batch_lists                = partition(keyword_list, nr_batches)\n    print(\"Phase 1: Collect the keywords with a search volume and select the keywords with a large volume (if nr_top_keywords is not None)\")\n\n    for batch in batch_lists:\n        batch                  = check_volume(batch, country_code, period, google_cat)\n        ordered_batch         = get_relative_rank(batch, country_code, period, google_cat)\n        if nr_top_keywords:\n            keywords_with_volume = keywords_with_volume + ordered_batch[0:nr_top_keywords]\n        else:\n            keywords_with_volume = keywords_with_volume + ordered_batch\n        \n     \n    if limit_final_list:\n        keywords_with_volume = keywords_with_volume[0:limit_final_list]        \n    \n    print(\"Phase 2: Get raw Google data for the keywords with large volume\")\n    print(\"The list of keywords with volume is \", str(keywords_with_volume))\n    if nr_batches > 1:                                                \n        ordered_search_terms = get_relative_rank(keywords_with_volume, country_code, period, google_cat)\n    else:\n        ordered_search_terms = keywords_with_volume\n                                                    \n    raw_scores = get_raw_scores(ordered_search_terms, country_code, period, google_cat)\n    raw_scores_spark_df = pandas_to_spark(raw_scores)\n    return raw_scores_spark_df"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["raw_google_trends_data_future_50_foods = get_raw_google_trends_batch_search(df_future_50_foods, \n                                                                            input_keyword_column, \n                                                                            input_country_code, \n                                                                            input_gt_period, \n                                                                            input_gt_category, \n                                                                            nr_batches, \n                                                                            nr_top_keywords, \n                                                                            limit_final_list)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["display(raw_google_trends_data_future_50_foods)"],"metadata":{},"outputs":[],"execution_count":6}],"metadata":{"name":"GoogleTrends_Future50Foods","notebookId":2729488883030443},"nbformat":4,"nbformat_minor":0}
